'Ankle boot')
str(class_names)
dim(train_images) # 60000 images with 28 x 28 Pixels
dim(train_labels) # 60000
train_images[1:2]
train_labels[1:20]
dim(test_images) # 10000 images with 28 x 28 Pixels
dim(test_labels) # 10000
train_images <- train_images / 255
test_images <- test_images / 255
## Plotting code ##
image_1 <- as.data.frame(train_images[1, , ])
colnames(image_1) <- seq_len(ncol(image_1))
image_1$y <- seq_len(nrow(image_1))
image_1 <- gather(image_1, "x", "value", -y)
image_1$x <- as.integer(image_1$x)
ggplot(image_1, aes(x = x, y = y, fill = value)) +
geom_tile() +
scale_fill_gradient(low = "white", high = "black", na.value = NA) +
scale_y_reverse() +
theme_minimal() +
theme(panel.grid = element_blank())   +
theme(aspect.ratio = 1) +
xlab("") +
ylab("")
# Display the first 25 images from the training set and display the class name below each image.
# Verify that the data is in the correct format and we’re ready to build and train the network.
par(mfcol=c(5,5))
par(mar=c(0, 0, 1.5, 0), xaxs='i', yaxs='i')
for (i in 1:25) {
img <- train_images[i, , ]
img <- t(apply(img, 2, rev))
image(1:28, 1:28, img, col = gray((0:255)/255), xaxt = 'n', yaxt = 'n',
main = paste(class_names[train_labels[i] + 1]))
}
model <- keras_model_sequential()
model %>%
# transforms the format of the images from a 2d-array (of 28 by 28 pixels), to a 1d-array of 28 * 28 = 784 pixels
# unstacking rows of pixels in the image and lining them up
layer_flatten(input_shape = c(28,28)) %>%
# dense layer to have 128 nodes (or neurons)
layer_dense(units = 128, activation = 'relu') %>%
# The second (and last) layer is a 10-node softmax layer —this returns an array of 10 probability scores that sum to 1.
# Each node contains a score that indicates the probability that the current image belongs to one of the 10 digit classes
layer_dense(units = 10, activation = 'softmax')
model %>%
# transforms the format of the images from a 2d-array (of 28 by 28 pixels), to a 1d-array of 28 * 28 = 784 pixels
# unstacking rows of pixels in the image and lining them up
layer_flatten(input_shape = c(28,28)) %>%
# dense layer to have 128 nodes (or neurons)
layer_dense(units = 128, activation = 'relu') %>%
# The second (and last) layer is a 10-node softmax layer —this returns an array of 10 probability scores that sum to 1.
# Each node contains a score that indicates the probability that the current image belongs to one of the 10 digit classes
layer_dense(units = 10, activation = 'softmax')
summary(model)
## Task 6: Compile the Model ##
model %>% compile(
optimizer = 'adam',
# categorical values ranges from 1 to 10 [chr [1:10] "T-shirt/top" "Trouser" "Pullover" "Dress" "Coat" "Sandal" "Shirt" "Sneaker" "Bag" "Ankle boot"]; So, Sparse_categorical_crossentropy is being used
loss = 'sparse_categorical_crossentropy',
metrics = c('accuracy')
)
model %>% fit(train_images, train_labels, epochs = 10, verbose = 2, validation_split=0.2)
# compare how the model performs on the test dataset
score <- model %>% evaluate(test_images, test_labels)
cat('Total Loss:',score[1],'\n')
cat('Total Accuracy:',score[2],'\n')
predictions <- model %>% predict(test_images)
predictions[1,]
which.max(predictions[1,])
class_pred <- model %>% predict_classes(test_images)
class_pred[1:20]
test_labels[1]
par(mfcol=c(5,5))
par(mar=c(0, 0, 1.5, 0), xaxs='i', yaxs='i')
for (i in 1:25) {
img <- test_images[i, , ]
img <- t(apply(img, 2, rev))
# subtract 1 as labels go from 0 to 9
predicted_label <- which.max(predictions[i, ]) - 1
true_label <- test_labels[i]
if (predicted_label == true_label) {
color <- '#008800'
} else {
color <- '#bb0000'
}
image(1:28, 1:28, img, col = gray((0:255)/255), xaxt = 'n', yaxt = 'n',
main = paste0(class_names[predicted_label + 1], " (",
class_names[true_label + 1], ")"),
col.main = color)
}
# Grab an image from the test dataset
# take care to keep the batch dimension, as this is expected by the model
img <- test_images[1, , , drop = FALSE]
dim(img)
predictions <- model %>% predict(img)
predictions
prediction <- predictions[1, ] - 1
which.max(prediction)
par(mfcol=c(5,5))
par(mar=c(0, 0, 1.5, 0), xaxs='i', yaxs='i')
for (i in 1:25) {
img <- test_images[i, , ]
img <- t(apply(img, 2, rev))
# subtract 1 as labels go from 0 to 9
predicted_label <- which.max(predictions[i, ]) - 1
true_label <- test_labels[i]
if (predicted_label == true_label) {
color <- '#008800'
} else {
color <- '#bb0000'
}
image(1:28, 1:28, img, col = gray((0:255)/255), xaxt = 'n', yaxt = 'n',
main = paste0(class_names[predicted_label + 1], " (",
class_names[true_label + 1], ")"),
col.main = color)
}
## Task 1: Import Libraries ##
EnsurePackage <- function(x){x <- as.character(x)
if (!require(x,character.only = TRUE)){
install.packages(pkgs=x, repos = "http://cran.us.r-project.org")
require(x, character.only = TRUE)
}
}
# usage example, to load the necessary library for further processing...
EnsurePackage("keras")
EnsurePackage("tidyr")
EnsurePackage("ggplot2")
cat("All Pacakges loaded")
## Task 2: Import the Fashion MNIST Dataset ##
fashion_mnist <- dataset_fashion_mnist()
c(train_images,train_labels) %<-% fashion_mnist$train
c(test_images,test_labels) %<-% fashion_mnist$test
class_names <- c('T-shirt/top',
'Trouser',
'Pullover',
'Dress',
'Coat',
'Sandal',
'Shirt',
'Sneaker',
'Bag',
'Ankle boot')
str(class_names)
## Task 3: Data Exploration ##
dim(train_images) # 60000 images with 28 x 28 Pixels
dim(train_labels) # 60000
train_images[1:2]
train_labels[1:20]
dim(test_images) # 10000 images with 28 x 28 Pixels
dim(test_labels) # 10000
## Task 4: Preprocess the Data ##
# library(tidyr)
# library(ggplot2)
## Normalization  to keep the pixel values fall in the range of 0 to 1 from 0 to 255:
train_images <- train_images / 255
test_images <- test_images / 255
## Plotting code ##
image_1 <- as.data.frame(train_images[1, , ])
colnames(image_1) <- seq_len(ncol(image_1))
image_1$y <- seq_len(nrow(image_1))
image_1 <- gather(image_1, "x", "value", -y)
image_1$x <- as.integer(image_1$x)
ggplot(image_1, aes(x = x, y = y, fill = value)) +
geom_tile() +
scale_fill_gradient(low = "white", high = "black", na.value = NA) +
scale_y_reverse() +
theme_minimal() +
theme(panel.grid = element_blank())   +
theme(aspect.ratio = 1) +
xlab("") +
ylab("")
# Display the first 25 images from the training set and display the class name below each image.
# Verify that the data is in the correct format and we’re ready to build and train the network.
par(mfcol=c(5,5))
par(mar=c(0, 0, 1.5, 0), xaxs='i', yaxs='i')
for (i in 1:25) {
img <- train_images[i, , ]
img <- t(apply(img, 2, rev))
image(1:28, 1:28, img, col = gray((0:255)/255), xaxt = 'n', yaxt = 'n',
main = paste(class_names[train_labels[i] + 1]))
}
## Task 5: Build the Model ##
model <- keras_model_sequential()
model %>%
# transforms the format of the images from a 2d-array (of 28 by 28 pixels), to a 1d-array of 28 * 28 = 784 pixels
# unstacking rows of pixels in the image and lining them up
layer_flatten(input_shape = c(28,28)) %>%
# dense layer to have 128 nodes (or neurons)
layer_dense(units = 128, activation = 'relu') %>%
# The second (and last) layer is a 10-node softmax layer —this returns an array of 10 probability scores that sum to 1.
# Each node contains a score that indicates the probability that the current image belongs to one of the 10 digit classes
layer_dense(units = 10, activation = 'softmax')
summary(model)
## Task 6: Compile the Model ##
model %>% compile(
optimizer = 'adam',
# categorical values ranges from 1 to 10 [chr [1:10] "T-shirt/top" "Trouser" "Pullover" "Dress" "Coat" "Sandal" "Shirt" "Sneaker" "Bag" "Ankle boot"]; So, Sparse_categorical_crossentropy is being used
loss = 'sparse_categorical_crossentropy',
metrics = c('accuracy')
)
## Task 7: Train and Evaluate the Model ##
model %>% fit(train_images, train_labels, epochs = 10, verbose = 2, validation_split=0.2)
# compare how the model performs on the test dataset
score <- model %>% evaluate(test_images, test_labels)
cat('Total Loss:',score[1],'\n')
cat('Total Accuracy:',score[2],'\n')
## Task 8: Make Predictions on Test Data ##
predictions <- model %>% predict(test_images)
predictions[1,]
which.max(predictions[1,])
class_pred <- model %>% predict_classes(test_images)
class_pred[1:20]
test_labels[1]
par(mfcol=c(5,5))
par(mar=c(0, 0, 1.5, 0), xaxs='i', yaxs='i')
for (i in 1:20) {
img <- test_images[i, , ]
img <- t(apply(img, 2, rev))
# subtract 1 as labels go from 0 to 9
predicted_label <- which.max(predictions[i, ]) - 1
true_label <- test_labels[i]
if (predicted_label == true_label) {
color <- '#008800'
} else {
color <- '#bb0000'
}
image(1:28, 1:28, img, col = gray((0:255)/255), xaxt = 'n', yaxt = 'n',
main = paste0(class_names[predicted_label + 1], " (",
class_names[true_label + 1], ")"),
col.main = color)
}
# Grab an image from the test dataset
# take care to keep the batch dimension, as this is expected by the model
img <- test_images[1, , , drop = FALSE]
dim(img)
predictions <- model %>% predict(img)
predictions
prediction <- predictions[1, ] - 1
which.max(prediction)
par(mfcol=c(5,5))
par(mar=c(0, 0, 1.5, 0), xaxs='i', yaxs='i')
for (i in 1:20) {
img <- test_images[i, , ]
img <- t(apply(img, 2, rev))
# subtract 1 as labels go from 0 to 9
predicted_label <- which.max(predictions[i, ]) - 1
true_label <- test_labels[i]
if (predicted_label == true_label) {
color <- '#008800'
} else {
color <- '#bb0000'
}
image(1:28, 1:28, img, col = gray((0:255)/255), xaxt = 'n', yaxt = 'n',
main = paste0(class_names[predicted_label + 1], " (",
class_names[true_label + 1], ")"),
col.main = color)
}
par(mfcol=c(5,5))
par(mar=c(0, 0, 1.5, 0), xaxs='i', yaxs='i')
for (i in 1:15) {
img <- test_images[i, , ]
img <- t(apply(img, 2, rev))
# subtract 1 as labels go from 0 to 9
predicted_label <- which.max(predictions[i, ]) - 1
true_label <- test_labels[i]
if (predicted_label == true_label) {
color <- '#008800'
} else {
color <- '#bb0000'
}
image(1:28, 1:28, img, col = gray((0:255)/255), xaxt = 'n', yaxt = 'n',
main = paste0(class_names[predicted_label + 1], " (",
class_names[true_label + 1], ")"),
col.main = color)
}
par(mfcol=c(5,5))
par(mar=c(0, 0, 1.5, 0), xaxs='i', yaxs='i')
for (i in 1:5) {
img <- test_images[i, , ]
img <- t(apply(img, 2, rev))
# subtract 1 as labels go from 0 to 9
predicted_label <- which.max(predictions[i, ]) - 1
true_label <- test_labels[i]
if (predicted_label == true_label) {
color <- '#008800'
} else {
color <- '#bb0000'
}
image(1:28, 1:28, img, col = gray((0:255)/255), xaxt = 'n', yaxt = 'n',
main = paste0(class_names[predicted_label + 1], " (",
class_names[true_label + 1], ")"),
col.main = color)
}
par(mfcol=c(5,5))
par(mar=c(0, 0, 1.5, 0), xaxs='i', yaxs='i')
for (i in 1:2) {
img <- test_images[i, , ]
img <- t(apply(img, 2, rev))
# subtract 1 as labels go from 0 to 9
predicted_label <- which.max(predictions[i, ]) - 1
true_label <- test_labels[i]
if (predicted_label == true_label) {
color <- '#008800'
} else {
color <- '#bb0000'
}
image(1:28, 1:28, img, col = gray((0:255)/255), xaxt = 'n', yaxt = 'n',
main = paste0(class_names[predicted_label + 1], " (",
class_names[true_label + 1], ")"),
col.main = color)
}
par(mfcol=c(5,5))
par(mar=c(0, 0, 1.5, 0), xaxs='i', yaxs='i')
for (i in 1:10) {
img <- test_images[i, , ]
img <- t(apply(img, 2, rev))
# subtract 1 as labels go from 0 to 9
predicted_label <- which.max(predictions[i, ]) - 1
true_label <- test_labels[i]
if (predicted_label == true_label) {
color <- '#008800'
} else {
color <- '#bb0000'
}
image(1:28, 1:28, img, col = gray((0:255)/255), xaxt = 'n', yaxt = 'n',
main = paste0(class_names[predicted_label + 1], " (",
class_names[true_label + 1], ")"),
col.main = color)
}
par(mfcol=c(5,5))
par(mar=c(0, 0, 1.5, 0), xaxs='i', yaxs='i')
for (i in 1:10) {
img <- test_images[i, , ]
img <- t(apply(img, 2, rev))
# subtract 1 as labels go from 0 to 9
predicted_label <- which.max(predictions[i, ]) - 1
true_label <- test_labels[i]
if (predicted_label == true_label) {
color <- '#008800'
} else {
color <- '#bb0000'
}
image(1:10, 1:10, img, col = gray((0:255)/255), xaxt = 'n', yaxt = 'n',
main = paste0(class_names[predicted_label + 1], " (",
class_names[true_label + 1], ")"),
col.main = color)
}
# Grab an image from the test dataset
# take care to keep the batch dimension, as this is expected by the model
img <- test_images[1, , , drop = FALSE]
dim(img)
predictions <- model %>% predict(img)
predictions
prediction <- predictions[1, ] - 1
which.max(prediction)
# import libraries
#create a function to ensure the libraries are imported
EnsurePackage <- function(x){
x <- as.character(x)
if (!require(x,character.only = TRUE)){
install.packages(pkgs=x, repos = "http://cran.us.r-project.org")
require(x, character.only = TRUE)
}
}
# usage example, to load the necessary library for further processing...
EnsurePackage("class")
EnsurePackage("e1071")
EnsurePackage("sqldf")
EnsurePackage("FactoMineR")
EnsurePackage("rpart.plot")
EnsurePackage("randomForest")
EnsurePackage("caret")
EnsurePackage("ggplot2")
EnsurePackage("dplyr")
EnsurePackage("RColorBrewer")
cat("All Packages are available")
EnsurePackage("class")
EnsurePackage("e1071")
EnsurePackage("sqldf")
EnsurePackage("FactoMineR")
EnsurePackage("rpart.plot")
EnsurePackage("randomForest")
EnsurePackage("caret")
EnsurePackage("ggplot2")
EnsurePackage("dplyr")
EnsurePackage("RColorBrewer")
cat("All Packages are available")
#Load CSV into data frame
train_data <- "/Users/sathishrajendiran/Documents/R/HW6/digit_train.csv"
digitTrainDF <- data.frame(read.csv(train_data,na.strings=c(""," ","NA")),stringsAsFactors=FALSE)
dim(digitTrainDF) #42000   785
digitTrainDF$label <- as.factor(digitTrainDF$label)
# Preview the structure
str(digitTrainDF)
# Analyze the spread
summary(digitTrainDF)
# Preview top few rows
head(digitTrainDF)
# 1. Training Set Preparation
set.seed(100)
# lets split the Training data set int into training and test datasets.creates a value for dividing the data into train and test.
# In this case the value is defined as   75% of the number of rows in the dataset
sample_size = floor(0.80*nrow(digitTrainDF)) # 65% --> 80% | 70% --> 82%  | 75 -->78% |80% -- 70%
# sample_size #value of the sample size 55
#
# set seed to ensure you always have same random numbers generated #324 has 100% training accuracy
train_index = sample(seq_len(nrow(digitTrainDF)),size = sample_size)
train_set =digitTrainDF[train_index,] #creates the training dataset with row numbers stored in train_index
# # table(train_data$author)
test_set=digitTrainDF[-train_index,]  # creates the test dataset excluding the row numbers mentioned in train_index
# # table(test_data$author)
#
cat("\nImages by Labels:")
table(digitTrainDF$label)
cat("\nTrain_set - Images by Labels:")
table(train_set$label)
cat("\nTest_set - Images by Labels:")
table(test_set$label)
# # compare number of images by labels
x <- data.frame(table(digitTrainDF$label))
colourCount = length(unique(digitTrainDF$label))
getPalette = colorRampPalette(brewer.pal(9, "Set1"))
gp_all <- ggplot(x, aes(x=Var1, y=Freq)) +   geom_bar(position="dodge",stat = "identity", fill=getPalette(colourCount))
gp_all <- gp_all + ggtitle("Train Data Images By Labels distribution") + xlab("Labels") + ylab("Number of Images")
gp_all <- gp_all +  theme(plot.title = element_text(hjust = 0.5),axis.title = element_text())
gp_all
# # compare number of images by labels
tn_set <- data.frame(table(train_set$label))
colourCount = length(unique(train_set$label))
getPalette = colorRampPalette(brewer.pal(9, "Set1"))
gp_tnset <- ggplot(tn_set, aes(x=Var1, y=Freq)) +   geom_bar(position="dodge",stat = "identity", fill=getPalette(colourCount))
gp_tnset <- gp_tnset + ggtitle("Train Set Images - Label distribution") + xlab("Labels") + ylab("Number of Images")
gp_tnset <- gp_tnset +  theme(plot.title = element_text(hjust = 0.5),axis.title = element_text())
gp_tnset
# # compare number of images by labels
tst_set <- data.frame(table(test_set$label))
colourCount = length(unique(test_set$label))
getPalette = colorRampPalette(brewer.pal(9, "Set1"))
gp_tst_set <- ggplot(tst_set, aes(x=Var1, y=Freq)) +   geom_bar(position="dodge",stat = "identity", fill=getPalette(colourCount))
gp_tst_set <- gp_tst_set + ggtitle("Test Set Images - Label distribution") + xlab("Labels") + ylab("Number of Images")
gp_tst_set <- gp_tst_set +  theme(plot.title = element_text(hjust = 0.5),axis.title = element_text())
gp_tst_set
#Load Test Data CSV as Validation data frame
validation_data <- "/Users/sathishrajendiran/Documents/R/HW6/digit_test.csv"
digitValidationDF <- data.frame(read.csv(validation_data,na.strings=c(""," ","NA")),stringsAsFactors=FALSE)
dim(digitValidationDF) #28000   784
digitValidationDF[,"label"] <- ''
digitValidationDF$label <- as.factor(digitValidationDF$label)
digitValidationDF <- digitValidationDF[,c(which(colnames(digitValidationDF)=="label"),which(colnames(digitValidationDF)!="label"))]
# Preview top few rows
head(digitValidationDF)
# Section 2: Build and tune decision tree models
# grow tree
rtree <- rpart(label~. ,data=train_set, method='class', cp=0,minsplit = 1, maxdepth = 5)
#summarize rtree values
summary(rtree)
plotcp(rtree) # plot cross-validation results
printcp(rtree) # plot cross-validation results
# Plot tree | lets Plot decision trees
rpart.plot(rtree,main="Classification Tree for MNISDigit Tokenizer", extra= 102) # plot decision tree
rsq.rpart(rtree) # plot approximate R-squared and relative error for different splits (2 plots)
# Section 3: Prediction | Test Phase
cat("\nTrain Data Images:")
table(digitTrainDF$label)
cat("\nTrain_Set - Images by Labels:")
table(train_set$label)
cat("\nTest_Set - Images by Labels:")
table(test_set$label)
predict_unseen <- predict(rtree, test_set, type = 'class')
# predict_unseen
table_test_matrix <- table(test_set$label, predict_unseen)
cat("\n\nPrediction results : Confusion Matrix \n\n")
# table_mat
confusionMatrix(table_test_matrix)
str(digitTrainDF)
str(test_set)
str(train_set)
str(digitValidationDF)
# Section 3: Prediction | Validation Phase
cat("\nTest Data Images:")
table(digitValidationDF$label)
cat("\nTrain Data Images:")
table(digitTrainDF$label)
cat("\nTrain_Set - Images by Labels:")
table(train_set$label)
cat("\nTest_Set - Images by Labels:")
table(test_set$label)
predict_final <- predict(rtree, digitValidationDF, type = 'class')
table_final <- table(digitValidationDF$label, predict_final)
cat("\n\nPrediction results : \n\n")
table_final
#View the final validation results and export to csv
predict_finaldf <- data.frame(predict_final)
cat("\n\nPrediction results by Label : \n\n")
View(predict_finaldf)
#Export to CSV
write.csv(x=predict_finaldf, file="/Users/sathishrajendiran/Documents/R/HW6/predict_finaldf.csv")
# Random Forest prediction of fedPapersDF1 data
EnsurePackage("randomForest")
# View(fedPapersDF1)
cat("\n All Articles by Author:")
table(test_set$label)
fit <- randomForest(y=test_set$label, x=test_set[2:ncol(test_set)], data=test_set, ntree=100
, keep.forest=FALSE, importance=TRUE)
print(fit) # view results
importance(fit) # importance of each predictor
rf_importance <- data.frame(importance(fit)) # importance of each predictor
rf_importance
training_kNN <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
set.seed(3033)
knn_fit <- train(label ~ ., data = train_set, method = "knn", trControl=training_kNN, preProcess = c("center", "scale"), tuneLength = 10)
